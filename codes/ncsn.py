#!/usr/bin/env python

"""
Noise Conditional Score Network
"""


import random

import numpy as np
from scipy.linalg import lstsq

from scipy.stats import rv_discrete, multivariate_normal, bernoulli
from scipy.special import expit, softmax
from sklearn.base import TransformerMixin

from cd_rbm import CDRBM

from sklearn.datasets import load_digits

from langevin import *

digists = load_digits()
X_train, y_train = digists.data, digists.target
n_samples, n_features = X_train.shape

import numpy as np  
from tensorflow.keras.models import Sequential  
from tensorflow.keras.layers import Dense  

input_dim = n_features + 1  
output_dim = n_features
 
model = Sequential([Dense(64, activation='relu', input_shape=(input_dim,)),
Dense(64, activation='relu'),
Dense(output_dim, activation='linear')]) 
model.compile(optimizer='adam', loss='mean_squared_error')  # 对于回归问题，我们使用均方误差作为损失函数  


for _ in range(10):
    sigma = np.random.random(1)+0.01
    ind = np.random.randint(n_samples, size=100)
    noise = np.random.normal(size=(100, n_features)) * sigma
    X_noised = X_train[ind] + noise
    X_noised = np.hstack((X_noised, np.full((100, 1), sigma)))
    model.fit(X_noised, noise/sigma, epochs=10, batch_size=32)


if __name__ == '__main__':
    
    class MyLangevinMC2(LangevinMC):

        def _like_rate(self, x, y):
            return 1

        def _grad_log_prob(self, x):
            p1 = multivariate_normal(mean1, cov1).pdf(x) * pi
            p2 = multivariate_normal(mean2, cov2).pdf(x) * (1-pi)
            return (np.dot(precise1, mean1-x) * p1 + np.dot(precise2, mean2-x) * p2) / (p1 + p2+0.0001)

    class MyAnnealingLangevinMC(MyLangevinMC2):

        def sample(self, init_state=None, size=1000, n_epoches=100, mask=None):

            size = size or self.n_steps
            state = self.init_state
            if init_state is None:
                state = self.init_state
            else:
                state = init_state

            if state is None:
                state = np.zeros(n_features)

            states = np.empty((size, n_features))
            cpy = state.copy()

            mc_size = size // n_epoches

            for k in range(n_epoches):
                sigma = 10 * 0.9**k
                alpha = 0.1 * 0.9 ** (k-n_epoches)
                self._log_prob = lambda x: model.predict(x, sigma)
                states[k*mc_size:(k+1)*mc_size] = super().sample(init_state=state, size=mc_size, sigma=alpha)
                state = states[(k+1)*mc_size-1]
                state[mask] = cpy[mask]

            return states


    ld = MyAnnealingLangevinMC(init_state=None, sigma=1)
    states = ld.sample(size=1000)

    # choose a sample
    x = X_train[4]
    xc = x.copy()
    mask = np.arange(64)<=32
    xc[~mask] = 0
    xr = states[-1]
    ld = MyAnnealingLangevinMC2(init_state=xc, sigma=1, mask=mask)
    states = ld.sample(size=1000)
    

    import matplotlib.pyplot as plt
    fig = plt.figure()
    ax = fig.subplots(2, 2)
    size = 8, 8  # size of image
    ax[0,0].imshow(x.reshape(size))
    ax[0,0].set_title('A real image')
    ax[0,1].imshow(xr.reshape(size))
    ax[0,1].set_title('Generated by the model')
    ax[1,0].imshow(xc.reshape(size))
    ax[1,0].set_title('A masked image')
    ax[1,1].imshow(x_.reshape(size))
    ax[1,1].set_title('Imputed by the model')
    for _ in ax.flat: _.set_axis_off()
    fig.suptitle("Demo of CD-RBM")
    plt.show()


    