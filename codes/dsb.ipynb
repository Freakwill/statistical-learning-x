{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Simplified Diffusion Schrodinger Bridge\n",
        "\n",
        "\n",
        "```bib\n",
        "@article{tang2024simplified,\n",
        "  title={Simplified Diffusion Schrodinger Bridge},\n",
        "  author={Tang, Zhicong and Hang, Tiankai and Gu, Shuyang and Chen, Dong and Guo, Baining},\n",
        "  journal={arXiv preprint arXiv:2403.14623},\n",
        "  year={2024}\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "eRPKc36VeYUH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsqP5BB5Wuhm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, UpSampling2D\n",
        "from tqdm.auto import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "seed = 123\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SThXbVvKWxGg"
      },
      "outputs": [],
      "source": [
        "ds = tfds.load('mnist')\n",
        "data = ds['train']\n",
        "\n",
        "# data = data.shuffle(60000, seed=seed).take(1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeIYLyzBi_vA"
      },
      "outputs": [],
      "source": [
        "def preprocess_fn(entry):\n",
        "    image = float(entry['image']) / 255.\n",
        "    image = tf.where(image > .5, 1.0, 0.0)\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    return image\n",
        "\n",
        "bs = 256\n",
        "train_data = data.map(preprocess_fn).shuffle(1024).batch(bs).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEMvMIEvndPI"
      },
      "outputs": [],
      "source": [
        "def corrupt(xs, gamma=0.01):\n",
        "    noise = tf.random.uniform(xs.shape)\n",
        "    gamma = tf.reshape(gamma, (-1, 1, 1, 1)) # Sort shape so broadcasting works\n",
        "    return xs + noise * gamma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTNHDgM2WfqD"
      },
      "outputs": [],
      "source": [
        "class BasicUNet(Model):\n",
        "    '''\n",
        "    net = BasicUNet()\n",
        "    x = tf.random.uniform((8, 28, 28, 1))\n",
        "    net(x).shape\n",
        "    # TensorShape([8, 28, 28, 1])\n",
        "    '''\n",
        "\n",
        "    def __init__(self, in_channels=1, out_channels=1):\n",
        "        super().__init__(name='basic-unet')\n",
        "        self.down_layers = [\n",
        "            Conv2D(32, kernel_size=5, padding=\"same\"),\n",
        "            Conv2D(64, kernel_size=5, padding=\"same\"),\n",
        "            Conv2D(64, kernel_size=5, padding=\"same\"),\n",
        "        ]\n",
        "        self.up_layers = [\n",
        "            Conv2D(64, kernel_size=5, padding=\"same\"),\n",
        "            Conv2D(32, kernel_size=5, padding=\"same\"),\n",
        "            Conv2D(out_channels, kernel_size=5, padding=\"same\"),\n",
        "        ]\n",
        "        self.act = tf.keras.activations.swish # also know as SiLU\n",
        "        self.downscale = MaxPool2D(2)\n",
        "        self.upscale = UpSampling2D(size=2)\n",
        "\n",
        "    def call(self, x, t=None):\n",
        "        h = []\n",
        "        for i, l in enumerate(self.down_layers):\n",
        "            x = self.act(l(x))\n",
        "            # Store x for skip connection and downscale for all but the third (final) down layer\n",
        "            if i < 2:\n",
        "              h.append(x)\n",
        "              x = self.downscale(x)\n",
        "\n",
        "        for i, l in enumerate(self.up_layers):\n",
        "            # Fetch x for skip connection and Upscale for all except the first up layer\n",
        "            if i > 0:\n",
        "              x = self.upscale(x)\n",
        "              x += h.pop()\n",
        "            x = self.act(l(x))\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8VStZfmZBkB"
      },
      "outputs": [],
      "source": [
        "bnet = BasicUNet()\n",
        "fnet = BasicUNet()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cPkycNRtWrqD",
        "outputId": "d64733b6-a0a3-4a6e-cb77-c128e38347b9"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'numpy.ndarray' object has no attribute 'map'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-17efc1b34074>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mdataset_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     normal_data = tf.data.Dataset.from_tensor_slices(\n\u001b[1;32m     41\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'map'"
          ]
        }
      ],
      "source": [
        "T=5\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "# Define a loss finction\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "losses, avg_losses = [], []\n",
        "\n",
        "train_data = data.map(preprocess_fn).shuffle(1024).batch(bs).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    for t in range(T):\n",
        "        for step, xb in enumerate(train_data):\n",
        "            with tf.GradientTape() as tape:\n",
        "                if epoch == 0:\n",
        "                    noised_xb = corrupt(xb)\n",
        "                else:\n",
        "                    noised_xb = corrupt(fnet(xb))\n",
        "                loss = loss_fn(bnet(noised_xb), xb)\n",
        "\n",
        "            if step == 0:\n",
        "                noised_data = noised_xb\n",
        "            else:\n",
        "                noised_data = np.concatenate((noised_data, noised_xb))\n",
        "\n",
        "            grads = tape.gradient(loss, bnet.trainable_weights)\n",
        "            opt.apply_gradients(zip(grads, bnet.trainable_weights))\n",
        "        train_data = noised_data\n",
        "\n",
        "        losses.append(loss.numpy())\n",
        "\n",
        "    # Calculate the average loss for this epoch\n",
        "    avg_loss = np.mean(losses[-len(xb):])\n",
        "    avg_losses.append(avg_loss)\n",
        "\n",
        "\n",
        "    dataset_size = len(list(data.map(lambda x: x['image'])))\n",
        "    prior_data = tf.data.Dataset.from_tensor_slices(\n",
        "        np.random.uniform(size=(dataset_size, 28, 28, 1))\n",
        "    )\n",
        "    train_data = data.map(preprocess_fn).shuffle(1024).batch(bs).prefetch(tf.prior_data.AUTOTUNE)\n",
        "    for t in range(T):\n",
        "        for step, xb in enumerate(prior_data):\n",
        "            with tf.GradientTape() as tape:\n",
        "                noised_xb = corrupt(bnet(xb))\n",
        "                loss = loss_fn(fnet(noised_xb), xb)\n",
        "            if step == 0:\n",
        "                noised_data = noised_xb\n",
        "            else:\n",
        "                noised_data = np.concatenate((noised_data, noised_xb))\n",
        "\n",
        "            grads = tape.gradient(loss, fnet.trainable_weights)\n",
        "            opt.apply_gradients(zip(grads, fnet.trainable_weights))\n",
        "        priori_data = noised_data\n",
        "        losses.append(loss.numpy())\n",
        "\n",
        "    # Calculate the average loss for this epoch\n",
        "    avg_loss = np.mean(losses[-len(xb):])\n",
        "    avg_losses.append(avg_loss)\n",
        "\n",
        "# View the loss curve\n",
        "plt.plot(losses)\n",
        "plt.ylim(0, 0.1);\n",
        "\n",
        "# Fetch some data (using the first 8 for easy plotting)\n",
        "batch = data.map(lambda x: x['image']).take(8)\n",
        "xs = tf.convert_to_tensor(list(batch), dtype=tf.float32)\n",
        "\n",
        "# Corrupt the images with a range of amounts\n",
        "amount = tf.linspace(0.0, 1.0, xs.shape[0])\n",
        "noised_xs = corrupt(xs, amount)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjNfsc6jo6aL"
      },
      "outputs": [],
      "source": [
        "\n",
        "plt.plot(losses)\n",
        "plt.ylim(0, 0.1);\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyW8ywX_-k8T"
      },
      "outputs": [],
      "source": [
        "def make_grid(xs, rows=1, cols=8):\n",
        "    xs = xs.numpy().squeeze()\n",
        "    images = [Image.fromarray(x) for x in xs]\n",
        "    return image_grid(images, rows, cols, 'L')\n",
        "\n",
        "def image_grid(imgs, rows, cols, mode='RGB'):\n",
        "    assert len(imgs) == rows*cols\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new(mode, size=(cols*w, rows*h))\n",
        "    grid_w, grid_h = grid.size\n",
        "\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "    return grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdoWdqVpWo1l"
      },
      "outputs": [],
      "source": [
        "n_steps = 15\n",
        "\n",
        "# Start from random noise\n",
        "x = tf.random.uniform((8, 28, 28, 1))\n",
        "step_history = [x]\n",
        "\n",
        "for t in range(n_steps, 0, -1):\n",
        "    # Predict the denoised x0\n",
        "    pred = bnet(x)\n",
        "    # How much we move towards the prediction\n",
        "    mix_factor = 1/(n_steps+1)\n",
        "    x = corrupt(pred)\n",
        "    # Store step for plotting\n",
        "    x = tf.clip_by_value(x, 0, 1)\n",
        "    step_history.append(x)\n",
        "\n",
        "fig, axs = plt.subplots(n_steps+1, 1, figsize=(8, 8), sharex=True)\n",
        "axs[0].set_xlabel('x(t)')\n",
        "axs[0].imshow(make_grid(step_history[0]), cmap='Greys')\n",
        "for i in range(1, n_steps+1):\n",
        "    axs[i].imshow(make_grid(step_history[i]), cmap='Greys')\n",
        "    axs[i].set_ylabel(f't=-{i}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmR-BJqsWpWu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}